This week two assignment shows how we can further improve/optimize our deep neural network using mini-batch gradient descent and an advanced optimization algorithm called "Adam's Optimization"
